#' Optimization of model parameters via simulated annealing
#'
#' [tune_sim_anneal()] uses models to generate new candidate tuning parameter
#'  combinations based on previous results. It uses the generalized simulated
#'  annealing method of Bohachevsky, Johnson, and Stein (1986).
#'
#' @param metrics A [yardstick::metric_set()] object containing information on how
#' models will be evaluated for performance. The first metric in `metrics` is the
#' one that will be optimized.
#' @param iter The maximum number of search iterations.
#' @param initial An initial set of results in a tidy format (as would result
#' from [tune_grid()]) or a positive integer. It is suggested that the number of
#' initial results be greater than the number of parameters being optimized.
#' @param control A control object created by [control_sim_anneal()]
#' @param ... Not currently used.
#' @details
#' Simulated annealing is a global optimization method. For model tuning, it
#'  can be used to iteratively search the parameter space for optimal tuning
#'  parameter combinations. At each iteration, a new parameter combination is
#'  created by perturbing the current parameters in some small way so that they
#'  are within a small neighborhood. This new parameter combination is used to
#'  fit a model and that model's performance is measured using resample (or a
#'  simple validation set).
#'
#' If the new settings have better results than the current settings, they are
#'  accepted and the process continues.
#'
#' If the new settings has worse performance, a probability threshold is
#'  computed for accepting these sub-optimal values. The probability is a
#'  function of _how_ sub-optimal the results are as well as how many iterations
#'  have elapsed. This is referred to as the "cooling schedule" for the
#'  algorithm. If the sub-optimal results are accepted, the next iterations
#'  settings are based on these inferior results. Otherwise, new parameter
#'  values are generated from the previous iteration's settings.
#'
#' This process continues for a pre-defined number of iterations and the
#'  overall best settings are recommended for use. The [control_sim_anneal()]
#'  function can specify the number of iterations without improvement for early
#'  stopping. Also, that function can be used to specify a _restart_ threshold;
#'  if no globally best results have not be discovered within a certain number
#'  if iterations, the process can restart using the last known settings that
#'  globally best.
#'
#' ## Creating new settings
#'
#' For each numeric parameter, the range of possible values is known as well
#'  as any transformations. The current values are transformed and scaled to
#'  have values between zero and one (based on the possible range of values). A
#'  candidate set of values that are on a sphere with radius `r` are generated.
#'  Infeasible values are removed and one value is chosen at random. This value
#'  is back transformed to the original units and scale and are used as the new
#'  settings. The argument `radius` of [control_sim_anneal()] controls the
#'  neighborhood size.
#'
#' For categorical parameters, each is changes with a pre-defined probability.
#'  The `flip` argument of [control_sim_anneal()] can be used to specify this
#'  probability.
#'
#' ## Cooling schedule
#'
#' To determine the probability of accepting a new value, the percent
#'  difference in performance is calculated. If the performance metric is to be
#'  maximized, this would be `d = (new-old)/old*100`. The probability is
#'  calculated as `p = exp(d * coef * iter)` were `coef` is a user-defined
#'  constant that can be used to increase or decrease the probabilities.
#'
#' The `cooling_coef` of [control_sim_anneal()] can be used for this purpose.
#'
#' ## Parallelism
#'
#' The `tune` and `finetune` packages currently parallelize over resamples.
#' Specifying a parallel back-end will improve the generation of the initial
#' set of sub-models (if any). Each iteration of the search are also run in
#' parallel as long as there are 2 or more resamples.
#'
#' @return A tibble of results that mirror those generated by [tune_grid()].
#' However, these results contain an `.iter` column and replicate the `rset`
#' object multiple times over iterations (at limited additional memory costs).
#' @references
#' Bohachevsky, Johnson, and Stein (1986) "Generalized Simulated Annealing for
#' Function Optimization", _Technometrics_, 28:3, 209-217
#' @examples
#' library(finetune)
#' library(rpart)
#' library(dplyr)
#' library(tune)
#' library(rsample)
#' library(parsnip)
#' library(workflows)
#' library(ggplot2)
#'
#' ## -----------------------------------------------------------------------------
#'
#' data(two_class_dat, package = "modeldata")
#'
#' set.seed(5046)
#' bt <- bootstraps(two_class_dat, times = 5)
#'
#' ## -----------------------------------------------------------------------------
#'
#' cart_mod <-
#'   decision_tree(cost_complexity = tune(), min_n = tune()) %>%
#'   set_engine("rpart") %>%
#'   set_mode("classification")
#'
#' ## -----------------------------------------------------------------------------
#'
#' # For reproducibility, set the seed before running.
#' set.seed(37275)
#' sa_search <-
#'   cart_mod %>%
#'   tune_sim_anneal(Class ~ ., resamples = bt, iter = 10)
#'
#' autoplot(sa_search, metric = "roc_auc", type = "parameters") +
#'   theme_bw()
#'
#' ## -----------------------------------------------------------------------------
#' # More iterations. `initial` can be any other tune_* object or an integer
#' # (for new values).
#'
#' set.seed(1970)
#' more_search <-
#'   cart_mod %>%
#'   tune_sim_anneal(Class ~ ., resamples = bt, iter = 10, initial = sa_search)
#'
#' @seealso [tune::tune_grid()], [control_sim_anneal()], [yardstick::metric_set()]
#' @export
tune_sim_anneal <- function(object, ...) {
  UseMethod("tune_sim_anneal")
}

#' @export
tune_sim_anneal.default <- function(object, ...) {
  msg <- paste0(
    "The first argument to [tune_sim_anneal()] should be either ",
    "a model or workflow."
  )
  rlang::abort(msg)
}

#' @export
tune_sim_anneal.recipe <- function(object,
                                   model,
                                   resamples,
                                   ...,
                                   iter = 10,
                                   param_info = NULL,
                                   metrics = NULL,
                                   initial = 1,
                                   control = control_sim_anneal()) {

  tune::empty_ellipses(...)

  tune_sim_anneal(model, preprocessor = object, resamples = resamples,
                  iter = iter, param_info = param_info,
                  metrics = metrics,  initial = initial, control = control)
}

#' @export
tune_sim_anneal.formula <- function(formula,
                                    model,
                                    resamples,
                                    ...,
                                    iter = 10,
                                    param_info = NULL,
                                    metrics = NULL,
                                    initial = 1,
                                    control = control_sim_anneal()) {

  tune::empty_ellipses(...)

  tune_sim_anneal(model, preprocessor = formula, resamples = resamples,
                  iter = iter, param_info = param_info,
                  metrics = metrics, initial = initial, control = control)
}

#' @export
#' @rdname tune_sim_anneal
tune_sim_anneal.model_spec <- function(object,
                                       preprocessor,
                                       resamples,
                                       ...,
                                       iter = 10,
                                       param_info = NULL,
                                       metrics = NULL,
                                       initial = 1,
                                       control = control_sim_anneal()) {

  if (rlang::is_missing(preprocessor) || !tune::is_preprocessor(preprocessor)) {
    rlang::abort(paste("To tune a model spec, you must preprocess",
                       "with a formula or recipe"))
  }

  tune::empty_ellipses(...)

  wflow <- workflows::add_model(workflow(), object)

  if (tune::is_recipe(preprocessor)) {
    wflow <- workflows::add_recipe(wflow, preprocessor)
  } else if (rlang::is_formula(preprocessor)) {
    wflow <- workflows::add_formula(wflow, preprocessor)
  }

  tune_sim_anneal_workflow(wflow, resamples = resamples, iter = iter,
                           param_info = param_info, metrics = metrics,
                           initial = initial, control = control, ...)
}


#' @export
#' @rdname tune_sim_anneal
tune_sim_anneal.workflow <-
  function(object,
           resamples,
           ...,
           iter = 10,
           param_info = NULL,
           metrics = NULL,
           initial = 1,
           control = control_sim_anneal()) {

    tune::empty_ellipses(...)

    tune_sim_anneal_workflow(object, resamples = resamples, iter = iter,
                             param_info = param_info, metrics = metrics,
                             initial = initial, control = control, ...)
  }

## -----------------------------------------------------------------------------


tune_sim_anneal_workflow <-
  function(object, resamples, iter = 10, param_info = NULL, metrics = NULL,
           initial = 5, control = control_sim_anneal()) {
    start_time <- proc.time()[3]

    tune::check_rset(resamples)
    y_names <- outcome_names(object)
    rset_info <- tune::pull_rset_attributes(resamples)

    metrics <- tune::check_metrics(metrics, object)
    metrics_name <- names(attr(metrics, "metrics"))[1]
    maximize <- attr(attr(metrics, "metrics")[[1]], "direction") == "maximize"

    if (is.null(param_info)) {
      param_info <- dials::parameters(object)
    }
    tune::check_workflow(object, check_dials = is.null(param_info), pset = param_info)

    unsummarized <-
      tune::check_initial(initial, param_info, object, resamples, metrics, control) %>%
      tune::new_iteration_results(
        parameters = param_info,
        metrics = metrics,
        outcomes = y_names,
        rset_info =  rset_info,
        workflow = object
      )
    mean_stats <- tune::estimate_tune_results(unsummarized)

    tune::check_time(start_time, control$time_limit)

    i <- 0 # In case things fail before iteration.

    on.exit({
      if (i < iter) {
        cli::cli_alert_danger("Optimization stopped prematurely; returning current results.")
      }
      out <- tune::new_iteration_results(unsummarized, param_info,
                                         metrics, y_names, rset_info, object)
      return(out)
    })

    cols <- tune::get_tune_colors()
    if (control$verbose) {
      rlang::inform(cols$message$info(paste("Optimizing", metrics_name)))
    }

    ## -----------------------------------------------------------------------------

    result_history <- initialize_history(unsummarized)

    best_param <-
      tune::select_best(unsummarized, metric = metrics_name) %>%
      dplyr::select(-.config)
    grid_history <- best_param
    current_param <- best_param
    global_param <- current_param

    existing_iter <- max(result_history$.iter)

    ## -----------------------------------------------------------------------------

    count_improve <- count_restart <- 0

    log_sa_progress(x = result_history, max_iter = iter, maximize = maximize, metric = metrics_name)

    # TODO iterations when initializing with a previous tune object start from 1 :-O

    for (i in (existing_iter + 1):(existing_iter + iter)) {
      new_grid <-
        new_in_neighborhood(current_param,
                            param_info,
                            radius = control$radius,
                            flip = control$flip)
      grid_history <- dplyr::bind_rows(grid_history, new_grid)

      res <-
        object %>%
        tune::tune_grid(
          resamples = resamples,
          grid = new_grid,
          metrics = metrics
        ) %>%
        dplyr::mutate(.iter = i)

      result_history <-
        result_history %>%
        update_history(res, i) %>%
        sa_decide(metric = metrics_name, maximize = maximize, coef = control$cooling_coef)

      m <- nrow(result_history)

      if (result_history$results[m] == "improvement") {
        current_param <- new_grid
        best_param <- new_grid
        count_improve <- 0

        if (result_history$global_best[m]) {
          global_param <- current_param
          count_restart <- 0
        }
      } else {
        count_improve <- count_improve + 1
        count_restart <- count_restart + 1
        if (result_history$results[m] == "accept") {
          current_param <- new_grid
        }
      }

      ## -----------------------------------------------------------------------------

      iter_info <- iter_since_x(result_history)
      if (count_restart >= control$restart) {
        result_history$results[m] <- "restart"
        current_param <- global_param
        count_restart <- 0
      }

      ## -----------------------------------------------------------------------------

      unsummarized <-
        dplyr::bind_rows(unsummarized, res) %>%
        tune::new_iteration_results(
          parameters = param_info,
          metrics = metrics,
          outcomes = y_names,
          rset_info =  rset_info,
          workflow = object

        )

      ## -----------------------------------------------------------------------------

      log_sa_progress(x = result_history, max_iter = iter, maximize = maximize,
                      metric = metrics_name)

      if (count_improve >= control$no_improve) {
        rlang::inform(
          cols$message$danger(
            paste0("Stopping; no improvments in ", no_improve, " iterations.")
          )
        )
        break()
      }
    }

    # TODO re-compute .configs?

    if (check_hidden_arg(control, "sa_history", TRUE)) {
      save(result_history, file = file.path(tempdir(), "sa_history.RData"))
    }
    # Note; this line is probably not executed due to on.exit():
unsummarized
  }





